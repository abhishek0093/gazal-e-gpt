{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishek/miniconda3/envs/gpt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "from dataclasses import dataclass\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "import glob \n",
    "import random\n",
    "import datetime\n",
    "\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# print(gc.collect())\n",
    "# print(torch.mps.empty_cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyGPTDataLoader:\n",
    "    def __init__(self, B, T, input_file_path, input_files_list):\n",
    "\n",
    "        self.input_file_path = input_file_path\n",
    "        self.input_files_list = input_files_list\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        with open(input_file_path, \"r\") as file : \n",
    "            data = file.read()\n",
    "        \n",
    "        self.enc = AutoTokenizer.from_pretrained(\"sarvamai/sarvam-1\")\n",
    "        tokens = self.enc.encode(data)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        self.current_position = 0\n",
    "\n",
    "\n",
    "        print(f\"total tokens {len(self.tokens)}\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
    "\n",
    "    def next_batch(self) : \n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        buff = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = buff[:-1].view(B, T)\n",
    "        y = buff[1:].view(B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "\n",
    "        if self.current_position+B*T+1 > len(self.tokens) : \n",
    "            \n",
    "            random.shuffle(self.input_files_list)\n",
    "\n",
    "            with open(self.input_file_path, 'w') as output_file : \n",
    "                output_file.truncate(0)\n",
    "                #TODO : We could add stopwords after every document, to indicate model that this is different. \n",
    "                for input_files_path in self.input_files_list : \n",
    "                    with open(input_files_path, 'r') as input_file : \n",
    "                        data = input_file.read()\n",
    "                        output_file.write(data)\n",
    "\n",
    "            with open(self.input_file_path, 'r') as file : \n",
    "                data = file.read()\n",
    "            tokens = self.enc.encode(data)\n",
    "            self.tokens = torch.tensor(tokens)\n",
    "            self.current_position = 0\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyGPTConfig:\n",
    "    n_ctx : int = 1024\n",
    "    vocab_size : int = 50257\n",
    "    n_embed : int = 768\n",
    "    n_head : int = 12\n",
    "    n_layer : int = 12\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module) :\n",
    "    def __init__(self, config) :\n",
    "        self.config = config\n",
    "        super().__init__()\n",
    "\n",
    "        # We divide n_embed into n_heads metrices to calculate attention q,k,v metrices\n",
    "        assert config.n_embed % config.n_head == 0\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3*config.n_embed)\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.c_proj.MYGPT_SCALE_INIT = 1\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_ctx, config.n_ctx))\n",
    "                             .view(1, 1, config.n_ctx, config.n_ctx))\n",
    "\n",
    "    def forward(self, x) :\n",
    "        B, T, C = x.size()  # Batch size, token length, n_embed\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        q, k, v = qkv.split(self.config.n_embed, dim=2)\n",
    "        q = q.view(B, T, self.config.n_head, C//self.config.n_head).transpose(1, 2)  ## Dimension = (B, n_head, T, n_embed // n_head)\n",
    "        k = k.view(B, T, self.config.n_head, C//self.config.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.config.n_head, C//self.config.n_head).transpose(1, 2)\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # Setting is_casual = True automatically ensures masking and lower trianglular matrix structure\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by sir\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module) : \n",
    "    def __init__(self, config) : \n",
    "        super().__init__()\n",
    "\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4*config.n_embed)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4*config.n_embed, config.n_embed)\n",
    "        self.c_proj.MYGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x) : \n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTOneBlock(nn.Module) : \n",
    "    def __init__(self, config) : \n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embed)\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embed)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        # At attn stage they are going to exchange information with each other in respect to how interesing they find each other, while in MLP stage no exchange takes place and each thinks individually that what they found in themselves and other tokens in the attn interaction that they recently had. \n",
    "        # Residual pathways are important optimization step as they help to pass gradients from top to bottom so that bottom also gets something to improve upon. This helps mostly in very deep neural networks. \n",
    "        x = x + self.attn(self.ln_1(x))   # WE want a clear path of only pure 'x' to go all the way from inputs to output straight so that during backprop at this juction gradients get's distributed , and some of them go processed through these attn/MLP layers while ensuring some portion of it goes downward straight to the inputs. \n",
    "        x = x + self.mlp(self.ln_2(x))   # Continuing above, this is a type of optimization technique . \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class MyGPT2(nn.Module) : \n",
    "    def __init__(self, config) : \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embed), # Word-Token-Embedding (weights of the token embedding)\n",
    "            wpe = nn.Embedding(config.n_ctx, config.n_embed), #Word-position embedding (wts of postion embedding)\n",
    "            h = nn.ModuleList(GPTOneBlock(config) for _ in range(config.n_layer)), #This will contain all the hidden blocks repeated n_layers time. Each block contains layerNorm1, self attention_mechanism, layernorm2 and mlp. \n",
    "            ln_f = nn.LayerNorm(config.n_embed) #Gpt2 paper introduced a final layer norm to be added after all the attention blocks. \n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)   # The final language model head to project n_embed into n_vocab space. \n",
    "\n",
    "        # Implement weight sharing as shown in the paper\n",
    "            # Also saves 40M parameters learning. \n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self.__init_weights)\n",
    "\n",
    "    def __init_weights(self, module) : \n",
    "        #FIXME : Wte and lm_head are weight sharing , so they will be intialized twice. We could fix that. \n",
    "        if isinstance(module, nn.Linear) : \n",
    "            std = 0.02\n",
    "            if hasattr(module, \"MYGPT_SCALE_INIT\") : \n",
    "                std *= (2 * self.config.n_layer) ** -0.5 # NO. of residual layers is 2 x n_layers. Every single of layer has two pathwasys that add up -> MLP and attn. \n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "            if module.bias is not None : \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding) : \n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.01)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None) : \n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.n_ctx\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_embed = self.transformer.wpe(pos)\n",
    "        tok_embed = self.transformer.wte(idx)\n",
    "\n",
    "        x = pos_embed + tok_embed\n",
    "        for one_block in self.transformer.h : \n",
    "            # print(\"my_gpt_forward_for_loop\", x.size())\n",
    "            x = one_block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None : \n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences_to_generate = 5\n",
    "max_seq_length = 300\n",
    "B = 4\n",
    "T = 1024\n",
    "\n",
    "# Store train stats in csv file. \n",
    "logs_path = f\"logs_{datetime.datetime.today().date()}.csv\"\n",
    "open(logs_path, 'w').close()\n",
    "logs_file = open(logs_path, \"w+\")\n",
    "logs_file.write(\"step,loss,norm,lr,token_per_sec,time\\n\")\n",
    "logs_file.flush()\n",
    "logs_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGPT2(MyGPTConfig(vocab_size=50304))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(\"Model Loaded\")\n",
    "\n",
    "input_data_files = glob.glob(\"dataset/*/hi/*\")\n",
    "train_dataloader = MyGPTDataLoader(B, T, \"shayar.txt\", input_data_files)\n",
    "print(\"DataLoader Ready\")\n",
    "\n",
    "\n",
    "total_req_batch_size = 32768 \n",
    "assert total_req_batch_size % (B * T) == 0, \"we should fit\"\n",
    "grad_accum_steps = total_req_batch_size // (B * T)\n",
    "print(f\"TOtal batch simulation : {total_req_batch_size}, and it will be reached per {grad_accum_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom wrapper for the constant phase\n",
    "class ConstantLRScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, constant_lr, last_epoch=-1):\n",
    "        self.constant_lr = constant_lr\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.constant_lr for _ in self.base_lrs]\n",
    "    \n",
    "\n",
    "\n",
    "max_lr = 6e-4\n",
    "warmup_steps = 100\n",
    "total_var_lr_steps = 900\n",
    "constant_lr = 0.1 * max_lr\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=max_lr,  betas=(0.9, 0.95), device_type=device)\n",
    "\n",
    "# Combine schedulers in SequentialLR\n",
    "lr_scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[LinearLR(optimizer, start_factor=0.001, end_factor=1.0, total_iters=warmup_steps), \n",
    "                CosineAnnealingLR(optimizer, T_max=total_var_lr_steps - warmup_steps, eta_min=constant_lr), \n",
    "                ConstantLRScheduler(optimizer, constant_lr=constant_lr)],\n",
    "    milestones=[warmup_steps, total_var_lr_steps],  # Transition points\n",
    ")\n",
    "\n",
    "steps = []\n",
    "lr_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(total_train_steps + 1000) :\n",
    "    optimizer.zero_grad()\n",
    "    t0 = time.time()\n",
    "    loss_accum = 0\n",
    "    for grad_accum_step in range(grad_accum_steps): \n",
    "\n",
    "        x, y = train_dataloader.next_batch()\n",
    "        x, y = x.to(device), y.to(device) \n",
    "        logits, loss = model(x, y)\n",
    "        loss /= grad_accum_steps\n",
    "        loss.backward()\n",
    "\n",
    "        loss_accum += loss.detach()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    torch.mps.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    steps.append(step)\n",
    "    lr_list.append(lr_scheduler.get_last_lr())\n",
    "\n",
    "    #NOTE : Maybe per 100 more suitable\n",
    "    if step % 200 == 0: \n",
    "        checkpoint = {\n",
    "            \"model\" : model.state_dict()}\n",
    "\n",
    "        torch.save(checkpoint, f\"model_{step}.ckpt\")\n",
    "        print(f\"step: {step} | loss:{loss_accum.item():.4f} | norm : {norm:.4f} | lr : {lr_scheduler.get_last_lr()[0]} | token_per_sec = {(train_dataloader.B * train_dataloader.T * grad_accum_steps) / (t1-t0)}) | time : {t1-t0}\")\n",
    "\n",
    "\n",
    "    #Write Fetched \n",
    "    with open(logs_path, \"a\") as logs_file:\n",
    "        logs_file.write(f\"{step},{loss_accum.item():.4f},{norm:.4f},{lr_scheduler.get_last_lr()[0]},{(train_dataloader.B * train_dataloader.T * grad_accum_steps) / (t1-t0)},{t1-t0}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO : \n",
    "#     # Torch.autocast () for loss thing into fp.16 --> Not supported mac. Recent push. \n",
    "#     # Enable torch.complile for model. But currently not supported for MPS backend. \n",
    "\n",
    "\n",
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = {\"model\" : model.state_dict()}\n",
    "# torch.save(checkpoint, f\"model_{step}.ckpt\")\n",
    "# print(f\"step: {step} | loss:{loss_accum.item():.4f} | norm : {norm:.4f} | lr : {lr_scheduler.get_last_lr()[0]} | token_per_sec = {(train_dataloader.B * train_dataloader.T * grad_accum_steps) / (t1-t0)}) | time : {t1-t0}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
