{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "from dataclasses import dataclass\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MyGPTConfig:\n",
    "    n_ctx : int = 1024\n",
    "    vocab_size : int = 50257\n",
    "    n_embed : int = 768\n",
    "    n_head : int = 12\n",
    "    n_layer : int = 12\n",
    "\n",
    "\n",
    "class MLP(nn.Module) : \n",
    "    def __init__(self, config) : \n",
    "        super().__init__()\n",
    "\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4*config.n_embed)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4*config.n_embed, config.n_embed)\n",
    "        self.c_proj.MYGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x) : \n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTOneBlock(nn.Module) : \n",
    "    def __init__(self, config) : \n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embed)\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embed)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        # At attn stage they are going to exchange information with each other in respect to how interesing they find each other, while in MLP stage no exchange takes place and each thinks individually that what they found in themselves and other tokens in the attn interaction that they recently had. \n",
    "        # Residual pathways are important optimization step as they help to pass gradients from top to bottom so that bottom also gets something to improve upon. This helps mostly in very deep neural networks. \n",
    "        x = x + self.attn(self.ln_1(x))   # WE want a clear path of only pure 'x' to go all the way from inputs to output straight so that during backprop at this juction gradients get's distributed , and some of them go processed through these attn/MLP layers while ensuring some portion of it goes downward straight to the inputs. \n",
    "        x = x + self.mlp(self.ln_2(x))   # Continuing above, this is a type of optimization technique . \n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module) :\n",
    "    def __init__(self, config) :\n",
    "        self.config = config\n",
    "        super().__init__()\n",
    "\n",
    "        # We divide n_embed into n_heads metrices to calculate attention q,k,v metrices\n",
    "        assert config.n_embed % config.n_head == 0\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3*config.n_embed)\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.c_proj.MYGPT_SCALE_INIT = 1\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_ctx, config.n_ctx))\n",
    "                             .view(1, 1, config.n_ctx, config.n_ctx))\n",
    "\n",
    "    def forward(self, x) :\n",
    "        B, T, C = x.size()  # Batch size, token length, n_embed\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        q, k, v = qkv.split(self.config.n_embed, dim=2)\n",
    "        q = q.view(B, T, self.config.n_head, C//self.config.n_head).transpose(1, 2)  ## Dimension = (B, n_head, T, n_embed // n_head)\n",
    "        k = k.view(B, T, self.config.n_head, C//self.config.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.config.n_head, C//self.config.n_head).transpose(1, 2)\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # Setting is_casual = True automatically ensures masking and lower trianglular matrix structure\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by sir\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MyGPT2(nn.Module) : \n",
    "    def __init__(self, config) : \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embed), # Word-Token-Embedding (weights of the token embedding)\n",
    "            wpe = nn.Embedding(config.n_ctx, config.n_embed), #Word-position embedding (wts of postion embedding)\n",
    "            h = nn.ModuleList(GPTOneBlock(config) for _ in range(config.n_layer)), #This will contain all the hidden blocks repeated n_layers time. Each block contains layerNorm1, self attention_mechanism, layernorm2 and mlp. \n",
    "            ln_f = nn.LayerNorm(config.n_embed) #Gpt2 paper introduced a final layer norm to be added after all the attention blocks. \n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)   # The final language model head to project n_embed into n_vocab space. \n",
    "\n",
    "        # Implement weight sharing as shown in the paper\n",
    "            # Also saves 40M parameters learning. \n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self.__init_weights)\n",
    "\n",
    "    def __init_weights(self, module) : \n",
    "        #FIXME : Wte and lm_head are weight sharing , so they will be intialized twice. We could fix that. \n",
    "        if isinstance(module, nn.Linear) : \n",
    "            std = 0.02\n",
    "            if hasattr(module, \"MYGPT_SCALE_INIT\") : \n",
    "                std *= (2 * self.config.n_layer) ** -0.5 # NO. of residual layers is 2 x n_layers. Every single of layer has two pathwasys that add up -> MLP and attn. \n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "            if module.bias is not None : \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding) : \n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.01)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None) : \n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.n_ctx\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_embed = self.transformer.wpe(pos)\n",
    "        tok_embed = self.transformer.wte(idx)\n",
    "\n",
    "        x = pos_embed + tok_embed\n",
    "        for one_block in self.transformer.h : \n",
    "            # print(\"my_gpt_forward_for_loop\", x.size())\n",
    "            x = one_block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None : \n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGPT2(MyGPTConfig(vocab_size=50304))\n",
    "model.load_state_dict(torch.load(\"model_800.ckpt\")[\"model\"])\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences_to_generate = 5\n",
    "max_seq_length = 100\n",
    "\n",
    "\n",
    "my_gpt_tokenizer = AutoTokenizer.from_pretrained(\"sarvamai/sarvam-1\")\n",
    "tokens = my_gpt_tokenizer.encode(\"चलो आज फिर चलते हैं\")\n",
    "\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "vectorized_tokens = tokens.unsqueeze(0).repeat(num_sentences_to_generate, 1).to(device)\n",
    "\n",
    "max_seq_length = 50\n",
    "while vectorized_tokens.size(1) < max_seq_length :\n",
    "    with torch.no_grad() :  \n",
    "        next_logits, loss = model(vectorized_tokens)\n",
    "\n",
    "        next_logits = next_logits[:, -1, :]\n",
    "        next_probs = F.softmax(next_logits, -1)\n",
    "        topk_probs, topk_indices = torch.topk(next_probs, 50, -1)\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        vectorized_tokens = torch.cat([vectorized_tokens, xcol], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0\n",
    "for sentence_encoded in vectorized_tokens: \n",
    "    sentence = my_gpt_tokenizer.decode(sentence_encoded.tolist())\n",
    "\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# चलो आज फिर चलते हैं सुनते थे दिल की आग उस की तबी दयारों पे हम भी अब दादे हैं तो जी बहकी रुफ़ाई है पयाम रख देंगी गुज़राज़ारे लोग मिले भी अब तक लेकिन एक दिलबर नहीं है उल्फ\n",
    "# <s> खुदा से ये गुजारिशમે साला<<reserved_token_3325>> Agency<<reserved_token_4075>> ਲੰਬਾ ਜਾਣਕਾਰੀ ਨਿਰਧਾਰਤ ਮੌਜੂਦsd റൂ females females AT ವ್ಯವಹ Illహంpret<<reserved_token_2098>> dietsm गोष्टी used વેપ ਗਵਰਨਰસંગતitory ವ್ಯವಹನಿಯನ್ गोंൃതിसंबंध British ਪਾਲ খেলেনটো ನಿರ್ವಹಿಸಲುसतनਗਤ<<reserved_token_2313>>ಬರ್ pictures\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
